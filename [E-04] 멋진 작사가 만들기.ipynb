{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcc2ac82",
   "metadata": {},
   "source": [
    "# Exploration_04 멋진 작사가 만들기\n",
    "\n",
    "\n",
    "## 목차\n",
    "  \n",
    "  - 데이터 준비 및 불러오기\n",
    "  - 데이터 전처리\n",
    "  - 평가 데이터셋 분리\n",
    "  - 인공지능 학습시키기\n",
    "  - 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8c50a4",
   "metadata": {},
   "source": [
    "### step 1. 데이터 준비 및 불러오기\n",
    "  - 데이터는 노드에 있는 실습(1) 데이터 다듬기에서 제공하는 데이터를 사용한다.\n",
    "  - glob을 활용해서 모든 txt를 불러오고 raw_corpus 리스트에 문장 단위로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de069493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\"]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "\n",
    "import glob\n",
    "import os, re\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a538c015",
   "metadata": {},
   "source": [
    "### step 2. 데이터 전처리 과정\n",
    "\n",
    "  - 공백인 문장과 화자가 표기된 문장을 제거해준다.\n",
    "  - 길이가 0인 문장은 공백인 문장으로 생각할 수 있다.\n",
    "  - 문장의 끝이 :로 끝나는 경우는 없기 때문에 문장의 끝이 :인 경우 화자가 표기된 문장으로 생각할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dedfac47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now I've heard there was a secret chord\n",
      "That David played, and it pleased the Lord\n",
      "But you don't really care for music, do you?\n",
      "It goes like this\n",
      "The fourth, the fifth\n",
      "The minor fall, the major lift\n",
      "The baffled king composing Hallelujah Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah Your faith was strong but you needed proof\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   \n",
    "    if sentence[-1] == \":\": continue  \n",
    "\n",
    "    if idx > 9: break   \n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa3f890",
   "metadata": {},
   "source": [
    "  - 텍스트 생성 모델도 단어 사전을 만들어줘야 한다.\n",
    "  - 문장을 일정한 기준으로 쪼개주는 과정을 토큰화(Tokenize)라고 한다.\n",
    "  - 가장 심플한 방법은 띄어쓰기를 기준으로 나눠주는 것이다.\n",
    "  - 토큰화 과정에서 발생하는 몇 가지 문제들이 있다.\n",
    "      1. Hi, my name is John. *(\"Hi,\" \"my\", ..., \"john.\" 으로 분리됨) - 문장부호  \n",
    "      2. First, open the first chapter. *(First와 first를 다른 단어로 인식) - 대소문자\n",
    "      3. He is a ten-year-old boy. *(ten-year-old를 한 단어로 인식) - 특수문자\n",
    "  - 문장부호 문제를 해결하기 위해서 문장부호 양쪽에 공백을 넣어준다.\n",
    "  - 대소문자 문제를 해결하기 위해서 모든 문자를 소문자로 바꿔준다.\n",
    "  - 특수문자 문제를 해결하기 위해서 특수문자들을 모두 제거해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c8d0d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "# 토큰화를 위한 데이터 전처리 정규 표현식\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 특수문자 양쪽에 공백을 넣고\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "    sentence = sentence.strip() # 다시 양쪽 공백을 지웁니다\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "    return sentence\n",
    "\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480b75c7",
   "metadata": {},
   "source": [
    "  - 위의 표현식을 통하면 지저분한 문장을 깔끔하게 정리할 수 있다.\n",
    "  - 위의 표현식을 통해서 <\"This @_is ;;;sample        sentence.\"> 문장이 깔끔하게 정리되서 출력된 것을 확인할 수 있다.\n",
    "  - 위에서 만든 표현식을 통해서 토큰화를 진행한 후 끝단어 <end>를 제거하면 소스 문장, 첫단어 <start>를 제거하면 타겟문장이 된다.\n",
    "  - 보통 자연어처리 과정에서 모델의 입력이 되는 문장을 소스문장(source sentence), 정답 역활을 하는 출력문장을 타겟문장(target sentence)이라고 하며 각각 x_train 과 y_train에 해당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3663fe84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정제된 문장 모으기\n",
    "\n",
    "corpus = []\n",
    "\n",
    "# 공백 혹은 화자가 입력된 문장은 제외하기\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    # 지나치게 긴 문장은 다른 문장들이 과도한 Padding을 가지게 한다.\n",
    "    # 그렇기 때문에 토큰의 개수가 15개가 넘는 문장은 제외시켜 준다.\n",
    "    if len(preprocessed_sentence.split(' ')) > 15 : continue \n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "# 정제된 문장 확인하기\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9add7918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4 ...    0    0    0]\n",
      " [   2   15 2967 ...    0    0    0]\n",
      " [   2   33    7 ...   46    3    0]\n",
      " ...\n",
      " [   2    4  118 ...    0    0    0]\n",
      " [   2  258  194 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f60e339c100>\n"
     ]
    }
   ],
   "source": [
    "# tf.keras.preprocessing.text.Tokenizer 패키지를 사용해서 정제된 데이터를 토큰화 해주어야 한다.\n",
    "# 토큰화 과정을 통해서 단어사전이 만들어지고 데이터가 숫자로 변환된다.\n",
    "# 이 과정을 벡터화라고 하며 숫자로 변환된 데이터를 텐서라고 한다.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, # 단어장의 크기를 12,000 이상으로 하라고 했기 때문에 12,000으로 설정해준다.\n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "400aecb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# 토큰화를 통해서 만들어진 사전이 어떻게 구성되어 있는지 확인할 수 있다.\n",
    "\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c065b4b1",
   "metadata": {},
   "source": [
    "  - 만들어진 사전의 인덱스를 통해서 2번이 start라는 것을 알 수 있다.\n",
    "  - 그렇기 때문에 각 문장의 시작이 start인 2로 시작하고 end인 3으로 끝나는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ba0fa6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    4   95  303   62   53    9  946 6263    3    0    0    0]\n",
      "[  50    4   95  303   62   53    9  946 6263    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  \n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaad62b",
   "metadata": {},
   "source": [
    "### step 3. 평가 데이터셋 분리\n",
    "\n",
    "  - Tensor로 변환된 데이터를 사이킷런의 train_test_split( ) 함수를 사용해서 분리해준다.\n",
    "  - 총 데이터의 20%를 평가 데이터셋으로 사용하라는 조건이 붙었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "402aeca0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124810, 14)\n",
      "Target Train: (124810, 14)\n",
      "Source Val: (31203, 14)\n",
      "Target Val: (31203, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                         tgt_input,\n",
    "                                                         test_size=0.2,\n",
    "                                                         random_state=24)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "print(\"Source Val:\", enc_val.shape)\n",
    "print(\"Target Val:\", dec_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d432971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 객체 생성\n",
    "# 텐서플로우는 텐서로 생성된 데이터를 이용해서 tf.data.Dataset 객체를 생성하는 방법을 사용한다.\n",
    "# tf.data.Dataset.from_tensor_slices() 메서드를 이용해서 tf.data.Dataset 객체를 생성한다.\n",
    "\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6e56276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
    "valset = valset.shuffle(BUFFER_SIZE)\n",
    "valset = valset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "valset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831b68ec",
   "metadata": {},
   "source": [
    "### step 4. 인공지능 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86739d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4979c8c",
   "metadata": {},
   "source": [
    "  - 위 코드에서 embedding_size는 벡터의 차원수, 즉 단어가 추상적으로 표현되는 크기로서 값이 커질수록 추상적인 특징을 잡아낼 수 있다.\n",
    "  - 하지만 충분한 데이터가 주어지지 않으면 옳바른 값을 얻을 수 없다.\n",
    "  - hidden_size는 모델에 얼마나 많은 일꾼을 둘 것인가로 이해할 수 있다.\n",
    "  - embedding_size와 hidden_size 조절을 통해서 모델을 수정해볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fdddf83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 97s 152ms/step - loss: 3.4797\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 82s 168ms/step - loss: 3.0138\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 82s 168ms/step - loss: 2.8530\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 82s 168ms/step - loss: 2.7314\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 82s 168ms/step - loss: 2.6268\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 82s 168ms/step - loss: 2.5323\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 82s 168ms/step - loss: 2.4448\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 82s 168ms/step - loss: 2.3611\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 82s 168ms/step - loss: 2.2814\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 82s 168ms/step - loss: 2.2042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa3700e79d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aae949",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 93s 187ms/step - loss: 2.1821 - val_loss: 2.5213\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 91s 186ms/step - loss: 2.0647 - val_loss: 2.5144\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 90s 185ms/step - loss: 1.9874 - val_loss: 2.5088\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 90s 185ms/step - loss: 1.9160 - val_loss: 2.4962\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 90s 185ms/step - loss: 1.8464 - val_loss: 2.4948\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 90s 184ms/step - loss: 1.7788 - val_loss: 2.4884\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 90s 184ms/step - loss: 1.7130 - val_loss: 2.4834\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 90s 184ms/step - loss: 1.6499 - val_loss: 2.4850\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 90s 184ms/step - loss: 1.5891 - val_loss: 2.4850\n",
      "Epoch 10/10\n",
      "466/487 [===========================>..] - ETA: 3s - loss: 1.5307"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, validation_data=valset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2a094487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91c220d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i m a survivor <end> '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i m\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "219d8c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you re the only one <end> '"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you re\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb027c7",
   "metadata": {},
   "source": [
    "  - embedding_size = 256 / hidden_size = 1024 설정으로 학습데이터의 loss값은 2.2 수준이었지만 val_loss값은 2.4 수준이었다.\n",
    "  - 현재 모델에서 문장 생성을 했을때는 위의 결과를 얻었지만 다른 여러 문장들을 생성했을때 말이 안되는 문장들이 더 많았다.\n",
    "  - val_loss값을 좀 더 낮추기 위해서 embedding_size값과 hidden_size값을 수정해서 다시 테스트를 해봐야 할 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3782d4de",
   "metadata": {},
   "source": [
    "### Step 4-1. 모델 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63ba2e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 512  # 256에서 512로 값을 수정해서 테스트해본다.\n",
    "hidden_size = 2048    # 1024에서 2048로 값을 수정해서 테스트해본다.\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a22aba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 248s 471ms/step - loss: 3.2861\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 230s 473ms/step - loss: 2.7436\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 231s 474ms/step - loss: 2.4347\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 231s 474ms/step - loss: 2.1315\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 231s 474ms/step - loss: 1.8419\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 230s 473ms/step - loss: 1.5825\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 231s 473ms/step - loss: 1.3678\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 231s 474ms/step - loss: 1.2032\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 231s 475ms/step - loss: 1.0917\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 231s 474ms/step - loss: 1.0263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f05c04a0160>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91ab5f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 254s 518ms/step - loss: 1.0367 - val_loss: 2.1955\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 252s 517ms/step - loss: 0.9801 - val_loss: 2.2399\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 252s 517ms/step - loss: 0.9625 - val_loss: 2.2616\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 252s 516ms/step - loss: 0.9513 - val_loss: 2.2767\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 252s 517ms/step - loss: 0.9432 - val_loss: 2.2932\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 252s 518ms/step - loss: 0.9366 - val_loss: 2.3026\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 252s 517ms/step - loss: 0.9312 - val_loss: 2.3203\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 252s 516ms/step - loss: 0.9269 - val_loss: 2.3256\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 251s 515ms/step - loss: 0.9226 - val_loss: 2.3321\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 252s 516ms/step - loss: 0.9191 - val_loss: 2.3474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f05ead86a60>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, validation_data=valset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2105a6e",
   "metadata": {},
   "source": [
    "  - embedding_size와 hidden_size의 값을 각각 512와 2048로 바꾸어서 테스트를 진행하였다.\n",
    "  - 하지만 원하는 val_loss 값인 2.2에는 못미치는 2.3의 값을 얻었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56c948a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 2048  # 256에서 350으로 값을 수정해서 테스트해본다.\n",
    "hidden_size = 4096    \n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26609b61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 945s 2s/step - loss: 2.2625 - val_loss: 2.4024\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 944s 2s/step - loss: 1.9316 - val_loss: 2.2877\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 944s 2s/step - loss: 1.6247 - val_loss: 2.2153\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 943s 2s/step - loss: 1.3580 - val_loss: 2.1811\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 943s 2s/step - loss: 1.1589 - val_loss: 2.1909\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 942s 2s/step - loss: 1.0407 - val_loss: 2.2165\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 942s 2s/step - loss: 0.9834 - val_loss: 2.2436\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 942s 2s/step - loss: 0.9560 - val_loss: 2.2520\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 941s 2s/step - loss: 0.9413 - val_loss: 2.2673\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 942s 2s/step - loss: 0.9312 - val_loss: 2.2875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f055448fe20>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, validation_data=valset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2665d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b231c51a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love <end> '"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5238f8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you love <end> '"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e45657",
   "metadata": {},
   "source": [
    "  - embedding_size와 hidden_size의 값을 각각 2048 / 4096으로 올려서 진행했다.\n",
    "  - 테스트 시간이 엄청 늘어나서 더 큰 폭으로 올리는 것은 지금 당장 어렵다고 판단했다.\n",
    "  - 원하는 val_loss값은 2.2 수준의 값을 얻었다.\n",
    "  - 하지만 문장을 만드는데 있어서 재대로 작동하지 않아서 모델을 다시 학습시켜서 테스트해야했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92b3ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 1024  \n",
    "hidden_size = 2048    \n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcf7c17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 285s 506ms/step - loss: 3.2096 - val_loss: 2.8442\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 263s 539ms/step - loss: 2.6750 - val_loss: 2.5961\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 265s 544ms/step - loss: 2.3470 - val_loss: 2.4212\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 265s 544ms/step - loss: 2.0271 - val_loss: 2.2874\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 265s 545ms/step - loss: 1.7284 - val_loss: 2.1947\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 266s 545ms/step - loss: 1.4707 - val_loss: 2.1443\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 265s 544ms/step - loss: 1.2663 - val_loss: 2.1241\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 266s 545ms/step - loss: 1.1220 - val_loss: 2.1280\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 266s 545ms/step - loss: 1.0359 - val_loss: 2.1483\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 266s 545ms/step - loss: 0.9910 - val_loss: 2.1708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5ff43972b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, validation_data=valset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7be1d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab0816b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b6814c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i like the way how you re touchin me <end> '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i like\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e280165",
   "metadata": {},
   "source": [
    "  - 이번에는 embedding_sze와 hidden_size의 값을 각각 1024와 2048로 낮춰서 진행하였다.\n",
    "  - 수치가 낮아진 만큼 작업에 걸리는 속도도 많이 단축되었다.\n",
    "  - val_loss값은 원하는 값보다 더 낮은 2.1의 값을 얻을 수 있었다.\n",
    "  - 특히 문장을 만드는데 있어서 val_loss 2.2 수준의 모델보다 훨씬 좋은 문장을 만드는 것을 확인할 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f66f1ad",
   "metadata": {},
   "source": [
    "### Step 5. 회고\n",
    "\n",
    "  - 이번 프로젝트는 시간과의 싸움이라고 생각될만큼 모델을 작동시키는데 많은 시간이 소요되었다.\n",
    "  - 하지만 재미있었던점은 모델의 하이퍼파라미터 변경을 통해서 각각 다른 값을 얻고 또 다른 값에 따라서 모델들의 성능이 차이를 보였다는 것이다.\n",
    "  - 여기서는 embedding_size와 hidden_size만 변경해주었지만 시간이 좀 더 있었다면 batch_size나 learning_rate, optimizer 변경도 시도해볼만하다고 생각한다.\n",
    "  - 또 이번 프로젝트를 진행하면서 영어가 아닌 한국어를 통해서도 작사 혹은 작문이 가능한지에 대한 궁금증이 생겼다.\n",
    "  - 하지만 구글링을 통해서도 파이썬을 통한 한국어 작문에 대한 내용을 발견하기 쉽지 않았다. 추후에 시간이 되면 찾아보면 좋을 거 같다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
